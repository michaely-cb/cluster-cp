Copyright 2002, Cerebras Systems, Inc. All rights reserved.

# THIS IS A WORK IN PROGRESS AND IS NOT COMPLETE

# Cerebras Cluster 100G L3 Networking

## TL;DR

1. The Cerebras Cluster 100G production network is a multi-tier spine-leaf network.
2. MemoryX switches and CS' form the leaf tiers.
3. SwarmX switches for the spine tiers.
4. Up to 12 SwarmX switches per spine tier group, with 12 interconnects from each leaf tier to match the bandwidth of a CS.
5. L3 network due to the mesh of connections.
6. BGP for route distribution between switches.
7. Network configuration tools to manage database and deploy nodes and switches.

## Intended Audience

This document assumes knowledge about Cerebras Clusters. Especially the different node types and relationships between them.

This document also assumes knowledge about L3 networking. This includes advanced policy routing, and the underlying mechanics of containers and VMs in the Linux kernel. This also includes scale-out L3 switch deployments.

Information about these topics may be provided but the goal isn't to teach them. It's to provide information to someone knowledgable in these topics about what specific choices were made for the Cerebras Cluster.

## Overview

The Cerebras Cluster 100G production network is a multi-tier spine-leaf network similar to a [Clos network](https://en.wikipedia.org/wiki/Clos_network).

The leaf tiers of the Cerebras Cluster are the MemoryX (Activation/Weight) switches and CS'.

MemoryX switches support MemoryX, user, worker, and management nodes. MemoryX switches also support uplinks out of the cluster 100G network.

A spine tier of the Cerebras Cluster is formed by up to 12 SwarmX (Broadcast/Reduce) switches. These switches support SwarmX nodes and CS data port connections.

The leaf tier forms a full mesh into adjoining spine tier.

A MemoryX switch connect to all of the SwarmX switches in the adjoining spine tier by dividing 12 connections between them.

Similarly, each CS connects to all of the SwarmX switches in the adjoining spine tier by divinding its 12 connections between them.

This allows each MemoryX switch to have a full CS' bandwidth into the adjoining spine tier. Newer CS models increase the number of connections and will require either a larger mesh or migrating to faster uplink interfaces (ex: 400GbE or 800GbE).

SwarmX nodes form tiers based on the number of ingress and egress ports. Multiple tiers of SwarmX nodes may exist on a single SwarmX switch tier. And SwarmX nodes may cross SwarmX switches based on their connectivity requirements.

Based on the number of CS' and size of the SwarmX switches, there may be multiple SwarmX switch tiers as well. For example, a 32 CS cluster requires 128 port SwarmX switches in order to have a single SwarmX switch tier. Using 64 port SwarmX switches requires multiple SwarmX switch tiers.

The performance goals of the cluster requires fully utilizing the bandwidth of all connections from the leaf tiers to the adjoining spine tiers. This excludes simple L2 configurations due to the limitations of the spanning tree protocol (STP) and requires L3 [ECMP](https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing) instead.

Individual switches have allocated subnets from a cluster-wide IPv4 prefix and switch to switch links are L3 links with invididual `/31` subnets.

All node route configurations are interface specific. Node interfaces have static policy and non-policy routes. The locally attached switch is the gateway for non-local traffic for that interface.

BGP is used to distribute routes between switches in the cluster and to distribute routes learned from exterior uplinks.

## L2 vs L3

### Spanning Tree

Spanning tree and its various extentions (ex: MSTP, RSTP) work by constructing a single path that covers all switches and disabling switch connections that are considered redundant.

Every link in a CS cluster is needed for performance and loss of links is only acceptable in a performance degraded scenario for a limited time. From a performance perspective, there are no redundant links.

This makes the Cerebras Cluster incompatible with using spanning tree to manage the topology of the 100G network.

L3 networking, or some other technology that goes beyond L2 networking is required.

### LAG/MLAG

Link aggregation combines # TODO

The width of a SwarmX spine tier may be up to 12 switches. Most vendor MLAG implementations support only two switches in any spine tier. Critically, both Arista and Mellanox have this limitation.

Cross compatibility between switches is not guaranteed with MLAG. This could force us to build clusters out of only one kind of switch which exposes us to high lead times and inventory problems. IPv4 routing and BGP are standardized and cross switch compatibility is the norm.

## L3 tiering solution

### RFC 7938

[RFC 7938](https://datatracker.ietf.org/doc/rfc7938/) provided guidance for how BGP was used in this implementation.

### MemoryX (Activation/Weight) tier

### SwarmX (Broadcast/Reduce) tiers

#### Single BR switch tier vs multiple BR node tiers

#### Multiple BR switch tiers

## The Cluster Environment

### Delegated 100G prefix

### Delegated BGP ASNs

### Exterior switches and cluster uplinks

### Default gateway

In the Cerebras, Colovore environment; the 1G management interface has the default gateway and is used to access resources other than storage outside of the cluster.

It is important to keep in mind that this is a property which is specific to Cerebras at Colovore. It may or may not be the case for any other customer at Colovore, any other Cerebras site, or any other customer and their site.

It was determined that having the default gateway in a policy route breaks Cilium due to the combination of eBPF, policy routes and firewall rules it uses.

Because of this, only one interface can be used for default gateway. Route metrics are used to ensure that if 100G requires a default gateway, then the system only uses that gateway when 100G is active.

As a result, once 100G with default gateway is active then 1G cannot be used.

Assymetric routing offers a possible way to keep using 1G but we encounter environments though where firewalling on the customer equipment prevents it.

### DNS, NTP, etc...

The L3 implementation is IP address based and does not directly interface with DNS.

The L3 100G environment may be the only path to these resources though in some deployments though.

## Additional topics

### OpenFlow

OpenFlow networking provides an alternative to L3 networking.

Since the control plane is centeralized, this may greatly simplify switch deployment and allow for broader switch vendor options.

Substantial engineering resources may be needed to prototype this network but it may be worth the investment in terms of how much time and effort it can save in deployment.

### Hiding the cluster network

Asking the customer for a prefix on their network is necessary in order to prevent address conflicts between devices in the cluster and exterior network resources.

Using routed uplinks on the 100G network provides flexibility in how we distribute load and consume external resources form within the cluster, and allows us to more closely match internal deployments.

Several challenges result from this:
- A high level of networking expertise on both the Cerebras and customer sides are necessary during the installation.
- The requested prefix size can be quite large based on the cluster size which places a burden on the customer IT allocations.

It would be ideal if the Cerebras Cluster 100G network were fully isolated from the customer environment.

Two topics are introduced in order to delve into the details of how to accomplish this: Exterior Network Resources, and Cluster Devices on the Perimeter.

#### Exterior Network Resources

An exterior network resource is a resource which is not part of the cluster but devices in the cluster communicate with it.

NFS, DNS, NTP servers, a "jump host", etc... are all examples of an exterior network resource.

It may not even adjoin the cluster and instead may be several router hops away (ex: another site, country, etc...).

For example, one customer environment places NFS servers several router hops away and requires the use of DNS to resolve server address.

#### Cluster Devices on the Perimeter

A device in the cluster is on the perimeter if, within a single networking stack it sends packets to or receives packets from exterior network resources and other resources within the cluster.

This can be nodes or switches, and includes devices that merely forward packets (even with NAT) as well as devices that originate traffic regardless of which network interfaces are used or location of the device.

For example, a worker node communicating with an external DNS server is a device on the perimeter.

Management traffic is included in this description (ex: ssh, dns, etc...). That means every node is a device on the perimeter in common deployments due to traffic on the 1G interface.

Importantly, a cluster device is on the perimeter if it has so much as one route table which has IP addresses of resources both interior and exterior to the cluster.

#### Eliminating routed cluster uplinks

The cluster uplinks require that the customer understand and agree to establish routes in their infrastructure. This can be a complex topic on its own and in the past reuired access to the customer's networking experts for ongoing conversations.

Of external network resources, only storage requires the 100G uplinks for performance reasons.

Of cluster devices on the perimeter, only user and worker nodes access storage over 100G.

A reasonably straightforward way to eliminate the routed cluster uplinks is to use a second network interface on the user and worker nodes to attach to the customer network.

This can optionally be done by plumbing a second VLAN through the MemoryX switches which participates in the customer network and converting the routed uplinks to simple VLAN uplinks.

#### Eliminating the cluster prefix

The cluster prefix is an agreement with the customer about how a part of their network will be used.

It is important to understand that no such agreement exists without one. The customer can do what they want when they want with their network. Even if we make a good guess at network ranges we can use during installation; the customer can break that guess at any time.

For every Cluster Device on the Perimeter, the following challenges represent the larger problem of address conflicts:
1. Communicate successfully with an exterior network resources, and simultaneously with a resource in the cluster, where both have the same IP address.
2. Communicate successfully with an exterior network resource which has the same IP address as an interface on the device itself.

Any proposed solution should prove that it handles both challenges in order to be considered possibly valid.

For example, a simple NAT device in front of the 100G cluster network fails both challenges. By passing packets between devices in the cluster and exterior network resources; the simple NAT device becomes a cluster device on the perimeter. It still has to contend with conflicting information in its route tables, resulting in failing both challenges.

There may be complex NAT devices, or configurations using containers and proxies on devices on the perimeter which do satisfy these challenges.
