#!/usr/bin/env bash

# GENERATED FILE - DO NOT EDIT

SCRIPT_PATH=$(dirname "$0")
cd "$SCRIPT_PATH"
source "../pkg-common.sh"

set -e

function add_grpc_annotations() {
  local ingresses=()
  mapfile -t -O "${#ingresses[@]}" ingresses < <(kubectl get ingress -lapp.kubernetes.io/instance=cluster-server -A -ojson |
    jq -r ".items[] | [.metadata.namespace, .metadata.name] | join(\",\")")
  mapfile -t -O "${#ingresses[@]}" ingresses < <(kubectl get ingress -ljob-name -A -ojson |
    jq -r ".items[] | [.metadata.namespace, .metadata.name] | join(\",\")")
  for ingress in ${ingresses[@]}; do
    local ingress_namespace=$(echo "$ingress" | cut -d',' -f1)
    local ingress_name=$(echo "$ingress" | cut -d',' -f2)
    if [[ -z $(kubectl get ingress "$ingress_name" -n "$ingress_namespace" -o json |
      jq -r ".metadata.annotations[\"nginx.ingress.kubernetes.io/proxy-connect-timeout\"] // empty") ]]; then
      echo "adding annotations to ingress '$ingress_name' in namespace '$ingress_namespace'"
      kubectl patch ingress "$ingress_name" -n "$ingress_namespace" --type json -p '[
        {"op": "add", "path": "/metadata/annotations/nginx.ingress.kubernetes.io~1proxy-connect-timeout", "value": "60"},
	      {"op": "add", "path": "/metadata/annotations/nginx.ingress.kubernetes.io~1proxy-read-timeout", "value": "43200"},
	      {"op": "add", "path": "/metadata/annotations/nginx.ingress.kubernetes.io~1proxy-send-timeout", "value": "43200"},
	      {"op": "add", "path": "/metadata/annotations/nginx.ingress.kubernetes.io~1client-body-timeout", "value": "43200"},
	      {"op": "add", "path": "/metadata/annotations/nginx.ingress.kubernetes.io~1grpc-next-upstream", "value": "off"}
      ]'
    fi
  done
}

# https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#known-limitations
# to workaround limitation of pod topology during the scale down case, explicitly check and balance if needed
function balance_nginx_distribution() {
  echo "Checking nginx pod distribution..."
  local pod_data=$(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx \
    --field-selector=status.phase=Running -o jsonpath='{range .items[*]}{.metadata.name}:{.spec.nodeName}:{.metadata.deletionTimestamp}{"\n"}{end}' 2>/dev/null)
  [[ -z "$pod_data" ]] && { echo "No running pods"; return; }
  # Filter out terminating pods (those with deletionTimestamp)
  pod_data=$(echo "$pod_data" | awk -F: '$3=="" {print $1":"$2}')
  [[ -z "$pod_data" ]] && { echo "No running pods"; return; }

  # Extract nodes and count
  local nodes=$(echo "$pod_data" | cut -d: -f2)
  local counts=$(echo "$nodes" | sort | uniq -c | sort -nr)
  echo "Distribution"
  echo "$counts"

  # Delete pods until each node has â‰¤1 pod
  while read -r count node; do
    [[ -z "$node" || $count -le 1 ]] && continue
    local pods_to_delete=$((count - 1))
    echo "Node $node has $count pods, deleting $pods_to_delete"
    for ((i=1; i<=pods_to_delete; i++)); do
      local victim=$(echo "$pod_data" | awk -F: -v node="$node" '$2==node {print $1; exit}')
      if [[ -n "$victim" ]]; then
        echo "Deleting $victim from $node ($i/$pods_to_delete)"
        kubectl -n ingress-nginx delete pod "$victim" --ignore-not-found --wait=false
        # Remove deleted pod from pod_data to avoid selecting it again
        pod_data=$(echo "$pod_data" | grep -v "^$victim:")
      fi
    done
  done <<< "$counts"
}

{% if remote_log_path %}
PSSH_MGMT "sh -c 'mkdir -p {{ remote_log_path }} && chown 101:101 {{ remote_log_path }}'"
{% endif %}

additional_args=""

if is_incremental_deploy; then
  CLUSTER_CONFIG=${INCREMENTAL_DIR}/incremental-cluster.yaml
  mgmt_data_ips=$(get_mgmt_node_data_ips)
  get_mgmt_node_ips
  mgmt_ips=$(cat ${MGMT_NODE_LIST} | paste -sd, -)
  EXTERNAL_IPS="$mgmt_ips,$mgmt_data_ips"
  additional_args+="--set controller.replicaCount=$(get_mgmt_node_count) "
elif [ -n "$MGMT_NODE_DATA_IPS" ]; then
  EXTERNAL_IPS="$MGMT_NODE_IPS,$MGMT_NODE_DATA_IPS"
  additional_args+="--set controller.replicaCount=$(get_mgmt_node_count) "
fi

has_data_net=$(kubectl get net-attach-def ${DEFAULT_NET_ATTACH} -n"${SYSTEM_NAMESPACE}" &>/dev/null && echo "true" || echo "false")
if has_multiple_mgmt_nodes; then
  if ! $has_data_net; then
    echo "ERROR ${DEFAULT_NET_ATTACH} network attachment definition not found but is required for multiple management nodes"
    exit 1
  fi

  # cleanup deprecated second nginx svc
  kubectl delete svc -n ingress-nginx ingress-nginx-vip --ignore-not-found

  # install IP tables SNAT rule to prevent hung connections - SW-105626
  DATA_PARENTNET=$(get_data_network)
  if [ -z "$DATA_PARENTNET" ]; then
    echo "failed to get 100G parent network range, check ${CLUSTER_CONFIG}::(v2Groups[0].vlans[0].parentnet or groups[0].switchConfig.parentnet)"
    exit 1
  fi
  MGMT_IP_MAP=$(kubectl get nodes -lk8s.cerebras.com/node-role-management -ojson |
                jq -r '.items[] | [.metadata.name, (.status.addresses[] | select(.type == "InternalIP") | .address )] | @csv' |
                tr -d '"' | tr '\n' ';')
  CRD_IP_MAP=$(kubectl get nodes -lk8s.cerebras.com/node-role-coordinator -ojson |
                jq -r '.items[] | [.metadata.name, (.status.addresses[] | select(.type == "InternalIP") | .address )] | @csv' |
                tr -d '"' | tr '\n' ';')
  sed "s|SET_DATA_PARENTNET|$DATA_PARENTNET|g" <nginx-snat-template.sh >nginx-snat.sh.tmp
  sed "s|SET_MGMT_IP_MAP|${MGMT_IP_MAP}${CRD_IP_MAP}|g" <nginx-snat.sh.tmp >nginx-snat.sh
  rm nginx-snat.sh.tmp
  chmod 755 nginx-snat.sh
  PSCP_MGMT nginx-snat.sh /usr/local/bin/
  chmod 644 nginx-snat.service
  PSCP_MGMT nginx-snat.service /lib/systemd/system/
  PSSH_MGMT "systemctl daemon-reload ; systemctl enable nginx-snat ; systemctl restart nginx-snat"
  sleep 1
  if ! PSSH_MGMT "systemctl status nginx-snat --no-pager"; then
    echo "failed to install nginx snat service"
    exit 1
  fi
else
  # keep simple for single-mgmt-node/kind cluster
  additional_args+="--set controller.service.externalTrafficPolicy=Cluster "
  # hc_port can't be set for cluster policy
  additional_args+="--set controller.service.healthCheckNodePort=null "
fi

vip=$(get_data_vip)
if [ -n "$vip" ] && helm -n kube-system status kube-vip-data &>/dev/null; then
  echo "Cluster has a VIP ($vip), adding to external IP list..."
  EXTERNAL_IPS="$vip,$EXTERNAL_IPS"
fi

if is_incremental_deploy; then
  echo "spec:
  externalIPs:" >$INCREMENTAL_DIR/ext_ip.patch
  EXT_IPS=$(echo $EXTERNAL_IPS | tr ',' ' ')
  for eip in $EXT_IPS; do
    echo "    - $eip" >>$INCREMENTAL_DIR/ext_ip.patch
  done
  kubectl patch service -n ingress-nginx ingress-nginx-controller --type=merge --patch-file $INCREMENTAL_DIR/ext_ip.patch
  kubectl patch deployment ingress-nginx-controller -n ingress-nginx -p "{\"spec\": {\"replicas\": $(get_mgmt_node_count)}}"
  kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m
  balance_nginx_distribution

  # restart new nginx pods to get around the missing multus ip issue due to a race condition during incremental deploy
  for pod in $(kubectl -n ingress-nginx get pod -lapp.kubernetes.io/name=ingress-nginx -ojson | jq -r ".items[] | .metadata.name"); do
    if ! kubectl -n ingress-nginx get pod $pod -ojson |
      jq -r '.metadata.annotations."k8s.v1.cni.cncf.io/network-status"' |
      jq -r '.[] | select(.interface == "net1") | .ips[0]' &>/dev/null; then
      echo "restart pod $pod due to missing multus ip"
      kubectl -n ingress-nginx delete pod $pod
    else
      echo "pod $pod has multus ip"
    fi
  done
  exit 0
fi

ingress_nginx_version=$(helm -n ingress-nginx list -ojson | jq -r ".[]|.app_version")

# Upgrading nginx controller from 1.3.0 to 1.12.1 requires some special handling due to a breaking change on server snippet.
# The grpc settings specified through server snippet don't work any more. The new settings are done through annotations,
# see https://github.com/kubernetes/ingress-nginx/pull/11258.
# To make sure there is no disruption for existing jobs, we add those annotations on all existing ingresses before the upgrade.
if [ "$ingress_nginx_version" = "1.3.0" ]; then
  add_grpc_annotations
fi

if ! kubectl get mutatingwebhookconfigurations add-ingress-annotation; then
  echo "add-ingress-annotation webhook should be created before nginx upgrade."
  exit 1
fi

additional_args+="--set controller.service.externalIPs={${EXTERNAL_IPS}} "

# add data net to ensure nginx can talk with CRD or other pods in 100G
# without it, if CRD is in a different node, data flow(return path) can be on 1G instead of 100G
if $has_data_net; then
  additional_args+="--set controller.podAnnotations.'k8s\.v1\.cni\.cncf\.io/networks'=${SYSTEM_NAMESPACE}/${DEFAULT_NET_ATTACH} "
fi

tar xfz ingress-nginx-4.12.1.tgz
# one off rebuild svc due to field immutable, can be removed later
hc_port=$(kubectl get svc -ningress-nginx ingress-nginx-controller -ojson | jq '.spec.healthCheckNodePort // empty' -r)
if [ -n "${hc_port}" ] && [ "${hc_port}" != "30254" ]; then
  kubectl delete svc -ningress-nginx ingress-nginx-controller --ignore-not-found
fi
helm upgrade ingress-nginx ./ingress-nginx \
  --install --wait --timeout 5m --debug \
  --namespace ingress-nginx --create-namespace \
  -f ./values.yaml -f ./values-override.yaml $additional_args \
  --set controller.image.registry=$registry_url --set controller.image.tag={{ ctrl_image.tag }} \
  --set controller.image.image={{ ctrl_image.short_repo }} \
  --set controller.image.digest="" \
  --set controller.image.digestChroot="" \
  --set controller.admissionWebhooks.patch.image.registry=$registry_url \
  --set controller.admissionWebhooks.patch.image.tag={{ webhook_image.tag }} \
  --set controller.admissionWebhooks.patch.image.image={{ webhook_image.short_repo }} \
  --set controller.admissionWebhooks.patch.image.digest=""

# Run post-upgrade balance check
kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m
balance_nginx_distribution

