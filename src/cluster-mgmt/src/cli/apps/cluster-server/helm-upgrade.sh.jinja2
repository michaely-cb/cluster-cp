#!/usr/bin/env bash

SCRIPT_PATH=$(dirname "$0")
cd "$SCRIPT_PATH"
SCRIPT_FULL_PATH=$(cd "$(dirname "$SCRIPT_PATH")" && pwd)/$(basename "$SCRIPT_PATH")

source "../pkg-common.sh"

set -e

if [ -z "$namespace" ]; then
  namespace={{ namespace }}
fi
system_namespace={{ system_namespace }}

if [ -z "$wsjob_log_preferred_storage_type" ]; then
  wsjob_log_preferred_storage_type={{ wsjob_log_preferred_storage_type }}
fi

if [ -z "$cached_compile_preferred_storage_type" ]; then
  cached_compile_preferred_storage_type={{ cached_compile_preferred_storage_type }}
fi

if [ -z "$system_deploy_force" ]; then
  system_deploy_force={{ system_deploy_force }}
elif [ "$system_deploy_force" != "True" ]; then
  system_deploy_force="False"
fi

if [ -z "$disable_user_auth" ]; then
  disable_user_auth={{ disable_user_auth }}
elif [ "$disable_user_auth" != "True" ]; then
  disable_user_auth="False"
fi

if [ -z "$use_isolated_dashboards" ]; then
  use_isolated_dashboards={{ use_isolated_dashboards }}
elif [ "$use_isolated_dashboards" != "True" ]; then
  use_isolated_dashboards="False"
fi

if [ -z "$disable_fabric_json_check" ]; then
  disable_fabric_json_check={{ disable_fabric_json_check }}
elif [ "$disable_fabric_json_check" != "True" ]; then
  disable_fabric_json_check="False"
fi

if [ -z "$cbcore_image" ]; then
  cbcore_image={{ cbcore_image }}
fi

function apply_user_auth_secret() {
  local namespace=$1
  local secret=$2

  cat <<EOM | $KUBECTL apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: kube-user-auth
  namespace: $namespace
stringData:
  secret: "$secret"
EOM
}

function setup_user_auth() {
  if ! $KUBECTL -n $system_namespace get secret kube-user-auth &>/dev/null; then
    num_bytes=512
    random_bytes=$(head -c $num_bytes /dev/urandom | base64)
    apply_user_auth_secret "$system_namespace" "$random_bytes"
  fi

  # For other namespaces, we make a copy of the secret
  if [ "$system_namespace" != "$namespace" ]; then
    if ! $KUBECTL -n $namespace get secret kube-user-auth &>/dev/null; then
      secret=$($KUBECTL -n $system_namespace get secret kube-user-auth -ojsonpath='{.data.secret}' | base64 --decode)
      apply_user_auth_secret "$namespace" "$secret"
    fi
  fi
}

# Start the config daemonset. This is needed only for system namespace deployment.
if [ "$namespace" = "$system_namespace" ]; then
  if $KUBECTL get ds -n ${system_namespace} cluster-server-config 2>/dev/null; then
    existing="true"
  fi
  $KUBECTL create configmap cluster-server-config -n $namespace \
    --from-file=./cluster-server-config.sh --dry-run=client -oyaml | $KUBECTL apply -f -
  SYSTEM_NAMESPACE=$system_namespace \
    bash -c "cat ./cluster-server-config.yaml | envsubst | $KUBECTL apply -f -"

  if is_incremental_deploy; then
    config_ds="./incr-config-ds.yaml.template"
  else
    config_ds="./config-ds.yaml.template"
  fi
  REMOTE_LOG_PATH={{ remote_log_path }} \
    SYSTEM_NAMESPACE=$system_namespace \
    TAG=$(get_image_version alpine-kubectl) \
    bash -c "cat ${config_ds} | envsubst | $KUBECTL apply -f -"
  if [ "$existing" == "true" ]; then
    # restart to ensure CM reload
    $KUBECTL rollout restart -n ${system_namespace} "ds/cluster-server-config"
  fi

  timeout="2m"
  echo "waiting maximum of ${timeout} for cluster-server config update to complete"
  if ! $KUBECTL rollout status -n ${system_namespace} "ds/cluster-server-config" --timeout=$timeout; then
    echo "Warning: not all cluster-server-config pod completed in $timeout. Continue"
  else
    echo "All cluster-server-config pods completed"
    $KUBECTL delete ds -n ${system_namespace} cluster-server-config --ignore-not-found
  fi
fi

# Set disable/enable customer worker depending on internet accessability
if ! yq -e '.properties.customWorker | has("disabled")' "${CLUSTER_PROPERTIES}" &>/dev/null; then
  update_image_build_status
fi

if [ "$disable_fabric_json_check" != "True" ]; then
  disable_fabric_json_check="$(get_disable_fabric_json_check)"
fi

# Internal deployments based on cs_cluster.py prefer to use NFS storage if preferred storage type not specified.
# Package-based deployments default to use local storage.
# We always fallback to local storage if NFS volume was not found.
workdir_logs_nfs_mounted_path=""
if [ "${wsjob_log_preferred_storage_type}" == "nfs" ]; then
  nfs_volumes=$(kubectl -n $namespace get cm cluster-server-volumes -o jsonpath='{.data}' | jq -r '[(to_entries[] | ({name: .key, spec: (.value | fromjson)})) | select(.spec.labels["workdir-logs"] == "true")]')
  if [ "$nfs_volumes" != "[]" ]; then
    workdir_logs_nfs_mounted_path=$(echo $nfs_volumes | jq -r '.[0].spec.containerPath')
  fi
fi

# Both internal deployments based on cs_cluster.py and package-based deployments prefer to use 
# Ceph in multi-mgmt clusters and local storage in single-mgmt clusters, if preferred storage type was not specified.
# We always fallback to local storage if NFS volume was not found.
cached_compile_nfs_mounted_path=""
if [ "${cached_compile_preferred_storage_type}" == "nfs" ]; then
  nfs_volumes=$(kubectl -n $namespace get cm cluster-server-volumes -o jsonpath='{.data}' | jq -r '[(to_entries[] | ({name: .key, spec: (.value | fromjson)})) | select(.spec.labels["cached-compile"] == "true")]')
  if [ "$nfs_volumes" != "[]" ]; then
    cached_compile_nfs_mounted_path=$(echo $nfs_volumes | jq -r '.[0].spec.containerPath')
  fi
fi

additional=""
additional+="--set wsjobLogsPreferredStorageType=${wsjob_log_preferred_storage_type} "
additional+="--set cachedCompilePreferredStorageType=${cached_compile_preferred_storage_type} "
additional+="--set wsjob.workdirLogsNfsMountedPath=${workdir_logs_nfs_mounted_path} "
additional+="--set wsjob.cachedCompileNfsMountedPath=${cached_compile_nfs_mounted_path} "
additional+="--set imageBuilder.disabled=$(get_disable_custom_worker) "
additional+="--set imageBuilder.supportedSidecarImages=$(echo {{ supported_sidecar_images }} | sed 's/,/\\,/g') "
additional+="--set disableFabricJsonCheck=$disable_fabric_json_check "
additional+="--set maxTrainFabricFreqMhz=$(get_max_train_fabric_freq_mhz) "

if has_multiple_mgmt_nodes; then
  additional+="--set wsjob.cachedCompileRoot=/pvc/compile_root "
  additional+="--set logExport.path=/pvc/log-export "
  additional+="--set debugviz.path=/pvc/debug-artifact "

  # TODO: Remove this block in rel-2.6
  # We typically set up the real subvolume during ceph deployment.
  if ! kubectl -n $system_namespace get pvc debug-artifact-static-pvc; then
    create_ceph_subvolume debug-artifact $(get_debug_artifact_pvc_size)
    create_ceph_static_pvc debug-artifact $system_namespace
  fi

  create_ceph_static_pvc cached-compile $namespace
  create_ceph_static_pvc log-export $namespace
  create_ceph_static_pvc debug-artifact $namespace

  # In multi-mgmt nodes, we should always use the management node role for scheduling.
  # - with mgmt/coord separation disabled, mgmt nodes are equivalent to coord nodes
  # - with mgmt/coord separation enabled, this allows sessions without dedicated coordinator nodes to be scheduled on mgmt nodes
  additional+="--set multiMgmtNodes=true "
  additional+="--set replicaCount=2 "
  additional+="--set mgmtNodeDataIp=$(get_data_vip) "
else
  additional+="--set wsjob.cachedCompileRoot=/n1/wsjob/compile_root "
  additional+="--set logExport.path=/n1/log-export "
  additional+="--set debugviz.path=/n1/debug-artifact "
  additional+="--set mgmtNodeDataIp=$MGMT_NODE_DATA_IP "

  # Only in multi-coordinator mode (where we only have one mgmt node), we would use the coordinator node role for scheduling.
  # The multi-coordinator mode was something we introduced before we had multi-mgmt node clusters.
  # As of rel-2.4, we still have not formally deprecated this mode, so we are keeping this handling.

  # Note that this handling does not work because we don't use node selectors anymore, we use node affinity CS-232
  # This is left in for historical context, as multi coordinator logic is to be removed at once and not incrementally
  if [[ "${system_namespace}" != "${namespace}" ]]; then
    sed -i "s/node-role-management/node-role-coordinator/" charts/values-override.yaml
  fi
fi

set_cbcore=""
if [ -n "$cbcore_image" ]; then
  set_cbcore="--set wsjob.image=$cbcore_image"
fi

# Set mgmt_host to add it to the ingress hosts list
{% if is_kind_cluster %}
mgmt_host="localhost"
{% else %}
mgmt_host=$(hostname)
{% endif %}

if [ "$system_deploy_force" == "True" ]; then
  additional+="--set systemUpdate=true "
else
  # If sdf flag is not passed, if nsr has deploy-restricted label then don't proceed
  deploy_restricted=$($KUBECTL get nsr "${namespace}" -ojson | jq '.metadata.labels | has("labels.k8s.cerebras.com/deploy-restricted")')
  if [[ "$deploy_restricted" == "true" ]]; then
    echo "Cluster-server deploy failed: Please don't deploy to deploy-restricted namespaces, coordinate with the session owner if needed"
    exit 1
  fi
fi

$KUBECTL delete clusterrole "${namespace}"-log-export-role --ignore-not-found
$KUBECTL delete clusterrolebinding "${namespace}"-log-export-rolebinding --ignore-not-found
$MKDIR -p ${TOOLS_DIR}
chmod a+r charts/log-export.sh
$CP charts/log-export.sh ${TOOLS_DIR}

create_system_admin_secret "$(get_admin_svc_user)" "$(get_admin_svc_password)" "$namespace"
update_system_mgmt_addrs

# Enable RoCE/RDMA in cluster-server if the plugin is up
if $KUBECTL get daemonset rdma-device-plugin -n kube-system; then
  additional+="--set enableRoCE=true "
fi

# Always set up the secret to simplify the usernode packaging.
setup_user_auth
if [ "$disable_user_auth" == "True" ]; then
  additional+="--set disableUserAuth=true "
fi

if [ "$use_isolated_dashboards" == "True" ]; then
  # System namespace cluster server should not use isolated dashboards.
  # Isolated dashboards are only supposed to use in user namespaces.
  if [[ "${system_namespace}" != "${namespace}" ]]; then
    additional+="--set enableIsolatedDashboards=true "
  fi
fi

additional+="--set compileSystemType=$(get_system_type) "

if [ $(yq '.nodes | length' ${CLUSTER_CONFIG}) -eq 1 ]; then
  echo "compile pod detected, disabling non-coordinator resource requests"
  additional+="--set disableNonCRDResourceRequests=true "
fi

if $KUBECTL get ingress -nprometheus prometheus-grafana; then
  grafana_url=$($KUBECTL get ingress -nprometheus prometheus-grafana -ojsonpath='{.spec.rules[0].host}')
  additional+="--set extraEnvs.GRAFANA_URL=${grafana_url} "
  additional+="--set metrics.enabled=true "
fi

# This memory estimate may not be accurate for user namespaces given the systems
# can be moved into or out of the session after the cluster server is deployed.
# In future, we should consider integrating with vertical pod autoscaler for better estimates.
SYSTEM_COUNT=$(get_system_count_for_session ${namespace})
if [ ! -d /kind ]; then
  MEM_MI=1024
  if ((MEM_MI < 30 * SYSTEM_COUNT)); then
    MEM_MI=$((30 * SYSTEM_COUNT))
  fi
  additional+="--set resources.limits.memory=${MEM_MI}Mi "
fi

if [ -d /kind ]; then
  # We're in a kind cluster, skip resource limits
  additional+="--set skipSystemMaintenanceResourceLimits='true' "
fi


if [ "$system_namespace" != "$namespace" ]; then
  if kubectl get nodes -l k8s.cerebras.com/namespace=$namespace,k8s.cerebras.com/node-role-coordinator= -o name --no-headers | grep -q .; then
    additional+="--set-string podAnnotations.cerebras/session-contains-crd-nodes=true "
  else
    additional+="--set-string podAnnotations.cerebras/session-contains-crd-nodes=false "
  fi
fi

# chmod is needed here for non-root deployment.
chmod a+r ./charts
chmod a+r ./charts/.helmignore

# wait for at most 1min in case of concurrent deployment race conditions
helm history cluster-server --namespace ${namespace} --max 3 2>/dev/null || true
for ((i = 0; i < 6; i++)); do
  status=$(helm status cluster-server --namespace ${namespace} -o json | jq -r '.info.status // empty')
  if echo "$status" | grep -q "pending"; then
    if [ $i == 5 ]; then
      kubectl logs deploy/cluster-server --namespace ${namespace} --tail 100 || true
      echo "$(date) warning: last deploy stuck in $status for 1min+, force cleanup and continue"
      helm uninstall cluster-server --namespace ${namespace} || true
      break
    fi
    echo "$(date) warning: last deploy stuck in $status, retry after 10s"
    sleep 10
  else
    break
  fi
done

$HELM upgrade cluster-server charts \
  --install --create-namespace --debug --namespace ${namespace} \
  --atomic --wait --timeout 5m --force \
  -f charts/values.yaml \
  -f charts/values-override.yaml $additional \
  --set system_namespace=$system_namespace \
  --set namespace=$namespace \
  --set systemCount=${SYSTEM_COUNT} \
  --set image.repository=$registry_url/{{ cluster_image.short_repo }} \
  --set image.tag={{ cluster_image.tag }} $set_cbcore \
  --set extraLabels.SEMANTIC_VERSION={{ semantic_version }} \
  --set extraEnvs.ALPINE_KUBECTL_TAG={{ alpine_kubectl_tag }} \
  --set extraEnvs.ALPINE_CONTAINERD_TAG={{ alpine_containerd_tag }} \
  --set extraEnvs.ALPINE_KUBE_USER_AUTH_TAG={{ alpine_kube_user_auth_tag }} \
  --set extraEnvs.SYSTEM_CLIENT_TAG={{ system_client_tag }} \
  --set ingress.clusterServerHosts="{cluster-server.$cluster_name.$service_domain,cluster-server.$service_domain,$mgmt_host}" \
  --set debugviz.ingress.debugVizHosts="{debug-viz.$cluster_name.$service_domain}"
