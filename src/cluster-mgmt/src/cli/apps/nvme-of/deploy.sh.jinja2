#!/usr/bin/env bash

SCRIPT_PATH=$(dirname "$0")
cd "$SCRIPT_PATH"
source "../pkg-common.sh"

set -eo pipefail

allow_reboot="{{ allow_reboot }}"
force="{{ force }}"
if [ "$force" == "false" ]; then
  echo "SW-156424: One scalability issue was discovered just before rel-2.4 was due."
  echo "Disabling the NVMe-oF deployment until the issue is resolved, unless the force option was specified."
  exit 0
fi

if [ -z "$hosts" ]; then
  # Example: hosts="sc-r10ra19-s17,sc-r10ra19-s18,sc-r10ra19-s19"
  hosts="{{ hosts }}"
fi
if [ -z "$namespace" ]; then
  # Example: namespace="qa-master"
  namespace="{{ namespace }}"
fi
if [ "$hosts" != "-" ] && [ "$namespace" != "-" ]; then
  echo "ERROR: Please specify either 'hosts' or 'namespace', not both."
  exit 1
fi

{% raw %}

inventory_yaml="${SCRIPT_PATH}/inventory.yaml"
cp inventory.yaml.template ${inventory_yaml}

if [ -n "$INCREMENTAL_DIR" ]; then
  incremental_deploy="true"
  src_cluster_dir="$INCREMENTAL_DIR"
  src_cluster_yaml="$INCREMENTAL_DIR/incremental-cluster.yaml"
  dest_cluster_dir="$CONFIG_DIR"
  dest_cluster_yaml="$CONFIG_DIR/incremental-cluster.yaml"
  cluster_yaml=$(cat ${src_cluster_yaml})
else
  incremental_deploy="false"
  src_cluster_dir="$CONFIG_DIR"
  src_cluster_yaml="$CONFIG_DIR/cluster.yaml"
  dest_cluster_dir="$CONFIG_DIR"
  dest_cluster_yaml="$CONFIG_DIR/cluster.yaml"
  cluster_yaml=$(kubectl -n job-operator get cm cluster -ojsonpath='{.data.clusterConfiguration\.yaml}')
  echo "${cluster_yaml}" > ${CLUSTER_CONFIG}
fi

src_targets_yaml="$CONFIG_DIR/nvme-targets.yaml"
dest_targets_yaml="$CONFIG_DIR/nvme-targets.yaml"
yq eval -i ".all.vars.incremental_deploy = $incremental_deploy" ${inventory_yaml}
yq eval -i ".all.vars.src_cluster_dir = \"$src_cluster_dir\"" ${inventory_yaml}
yq eval -i ".all.vars.src_cluster_yaml = \"$src_cluster_yaml\"" ${inventory_yaml}
yq eval -i ".all.vars.src_targets_yaml = \"$src_targets_yaml\"" ${inventory_yaml}
yq eval -i ".all.vars.dest_cluster_dir = \"$dest_cluster_dir\"" ${inventory_yaml}
yq eval -i ".all.vars.dest_cluster_yaml = \"$dest_cluster_yaml\"" ${inventory_yaml}
yq eval -i ".all.vars.dest_targets_yaml = \"$dest_targets_yaml\"" ${inventory_yaml}
yq eval -i ".all.vars.allow_reboot = $allow_reboot" ${inventory_yaml}

all_mgmt_nodes=$(yq -e '.nodes | map(select(
  .role == "management" and
  .properties.storage-type == "ceph")).[] | .name // ""' <<< "${cluster_yaml}")
all_coord_nodes=$(yq -e '.nodes | map(select(
  .role == "management" and
  .properties.storage-type != "ceph")).[] | .name // ""' <<< "${cluster_yaml}")
all_memoryx_nodes=$(yq -e '.nodes | map(select(.role == "memory")).[] | .name // ""' <<< "${cluster_yaml}")

if [ -z "${all_mgmt_nodes}" ]; then
  echo "This is a single-mgmt cluster. Skipping NVMe-oF deployment."
  exit
elif [ -z "${all_coord_nodes}" ]; then
  echo "This cluster does not contain any coordinator nodes. Skipping NVMe-oF deployment."
  exit
fi

# If `csctl get node` returns with error, it means we are undergoing deployment on a new cluster.
# Otherwise, we should make sure the nodes selected do not have any jobs running on them.
nodes_info=$(csctl get nodes -ojson 2>/dev/null || echo '{"items": []}')
all_memoryx_nodes_info=$(jq -cr '{items: [.items[] | select(.role == "memoryx")]}' <<< "$nodes_info")
# In the event a cluster does not have management and coordinator nodes separated, we should get nodes with both roles
mgmt_nodes_info=$(jq -cr '{items: [.items[] | select(.role == "management")]}' <<< "$nodes_info")
coord_nodes_info=$(jq -cr '{items: [.items[] | select(.role == "coordinator")]}' <<< "$nodes_info")
all_coord_nodes_info=$(jq -s '{"items": map(.items) | add}' <(echo "$mgmt_nodes_info") <(echo "$coord_nodes_info"))
if [ "$namespace" != "-" ]; then
  all_deployable_nodes=$(jq -s '{"items": map(.items) | add}' <(echo "$all_coord_nodes_info") <(echo "$all_memoryx_nodes_info"))
  hosts=$(jq -cr ".items[] | select(.session == \"$namespace\") | .meta.name" <<< "$all_deployable_nodes" | paste -sd, -)
  if [ -z "$hosts" ]; then
    echo "ERROR: No deployable nodes found in the '$namespace' namespace."
    exit 1
  fi
fi

coord_nodes=$(yq -e '.nodes | map(select(
  .role == "management" and
  .properties.storage-type != "ceph" and
  .properties.nvme-of-target-id == null)).[] | .name // ""' <<< "${cluster_yaml}")
if [ -n "$coord_nodes" ]; then
  num_coord_nodes=$(echo "${coord_nodes}" | wc -l)
  echo "$num_coord_nodes coordinator node(s) had yet to be configured as NVMe-oF targets:"
  echo "$coord_nodes"
else
  num_all_coord_nodes=$(echo "${all_coord_nodes}" | wc -l)
  echo "All $num_all_coord_nodes coordinator node(s) had been configured as NVMe-oF targets."
fi

memoryx_nodes=$(yq -e '.nodes | map(select(
  .role == "memory" and
  .properties.nvme-of-initiator == null)).[] | .name // ""' <<< "${cluster_yaml}")
if [ -n "$memoryx_nodes" ]; then
  num_memoryx_nodes=$(echo "${memoryx_nodes}" | wc -l)
  echo "$num_memoryx_nodes memoryx node(s) had yet to be configured as NVMe-oF initiators:"
  echo "$memoryx_nodes"
else
  num_all_memoryx_nodes=$(echo "${all_memoryx_nodes}" | wc -l)
  echo "All $num_all_memoryx_nodes memoryx node(s) had been configured as NVMe-oF initiators."
fi

selected_coord_nodes=""
selected_memoryx_nodes=""
if [ "$hosts" == "-" ]; then
  selected_coord_nodes=$(echo -e "${all_coord_nodes}" | paste -sd, - | sed 's/$/,/')
  selected_memoryx_nodes=$(echo -e "${all_memoryx_nodes}" | paste -sd, - | sed 's/$/,/')
else
  IFS=',' read -r -a input_nodes <<< "$hosts"
  for input_node in "${input_nodes[@]}"; do
    if grep -wq "${input_node}" <<< "${all_coord_nodes}"; then
      selected_coord_nodes+="${input_node},"
    elif grep -wq "${input_node}" <<< "${all_memoryx_nodes}"; then
      selected_memoryx_nodes+="${input_node},"
    else
      echo "WARNING: Skipping '${input_node}' as it is neither a coordinator node or a memoryx node."
    fi
  done
fi

all_nodes=$(kubectl get nodes)
if echo "${all_nodes}" | grep -wq "NotReady"; then
  not_ready_nodes=$(echo "${all_nodes}" | grep -w "NotReady" | awk '{print $1}')
fi

nodes_have_jobs_running=0
while IFS= read -r node; do
  if [ -z "$node" ]; then
    continue
  elif [ -n "$not_ready_nodes" ] && grep -wq "$node" <<< "$not_ready_nodes"; then
    yq eval -i ".all.children.not_ready_coordinator.hosts.$node = {}" ${inventory_yaml}
  else
    yq eval -i ".all.hosts.$node = {}" ${inventory_yaml}
    if grep -wq "$node" <<< "${selected_coord_nodes}"; then
      running_jobs="[]"
      if [ -n "$all_coord_nodes_info" ]; then
        running_jobs=$(echo "${all_coord_nodes_info}" | jq -crM -e --arg node "$node" '.items[] | select(.meta.name == $node) | .jobIds')
      fi
      if [ "$running_jobs" != "[]" ]; then
        echo "ERROR: Skipping '$node' as it has running job(s) ${running_jobs}."
        nodes_have_jobs_running=$((nodes_have_jobs_running+1))
      else
        yq eval -i ".all.children.coordinator.hosts.$node = {}" ${inventory_yaml}
      fi
    fi
  fi
done <<< "${all_coord_nodes}"

while IFS= read -r node; do
  if [ -z "$node" ]; then
    continue
  elif [ -n "$not_ready_nodes" ] && grep -wq "$node" <<< "$not_ready_nodes"; then
    yq eval -i ".all.children.not_ready_memory.hosts.$node = {}" ${inventory_yaml}
  else
    yq eval -i ".all.hosts.$node = {}" ${inventory_yaml}
    if grep -wq "$node" <<< "${selected_memoryx_nodes}"; then
      running_jobs="[]"
      if [ -n "$all_memoryx_nodes_info" ]; then
        running_jobs=$(echo "${all_memoryx_nodes_info}" | jq -crM -e --arg node "$node" '.items[] | select(.meta.name == $node) | .jobIds')
      fi
      if [ "$running_jobs" != "[]" ]; then
        echo "ERROR: Skipping '$node' as it has running job(s) ${running_jobs}."
        nodes_have_jobs_running=$((nodes_have_jobs_running+1))
      else
        yq eval -i ".all.children.memory.hosts.$node = {}" ${inventory_yaml}
      fi
    fi
  fi
done <<< "${all_memoryx_nodes}"

while IFS= read -r node; do
  if [ -z "$node" ]; then
    continue
  elif [ -n "$not_ready_nodes" ] && grep -wq "$node" <<< "$not_ready_nodes"; then
    yq eval -i ".all.children.not_ready_management.hosts.$node = {}" ${inventory_yaml}
  else
    yq eval -i ".all.hosts.$node = {}" ${inventory_yaml}
    yq eval -i ".all.children.management.hosts.$node = {}" ${inventory_yaml}
  fi
done <<< "${all_mgmt_nodes}"

if [ $nodes_have_jobs_running -gt 0 ]; then
  echo "ERROR: $nodes_have_jobs_running node(s) have job(s) running on them. Please ensure the job(s) are completed or canceled before redeploying."
  exit 1
fi

if ! kubectl -n job-operator get cm nvme-targets >/dev/null 2>&1; then
  kubectl apply -f nvme-targets-cm.yaml
fi

targets_yaml=$(kubectl -n job-operator get cm nvme-targets -ojsonpath='{.data.targets\.yaml}')
echo "${targets_yaml}" > "${src_targets_yaml}"

all_targets_populated_in_cache=1
existing_targets=$(yq -e '.nodes | map(select(.properties.nvme-of-target-id != null)).[] | .name // ""' <<< "${cluster_yaml}")
if [ -n "${existing_targets}" ]; then
  while IFS= read -r target_node; do
    if ! echo "$targets_yaml" | yq -e ". | has(\"$target_node\")" >/dev/null 2>&1; then
      echo "WARNING: Adding '$target_node' to the 'nvme-targets' configmap."
      target_id=$(yq -e ".nodes[] | select(.name == \"$target_node\") | .properties.nvme-of-target-id" <<< "${cluster_yaml}")
      yq ".[\"$target_node\"] = \"$target_id\"" -i "${src_targets_yaml}"
      all_targets_populated_in_cache=0
    fi
  done <<< "${existing_targets}"
fi

if [ $all_targets_populated_in_cache -ne 1 ]; then
  # The configmap should always be the source of truth
  kubectl -n job-operator create cm nvme-targets --from-file=targets.yaml="${src_targets_yaml}" --dry-run=client -oyaml | kubectl replace -f-
fi

echo "NVMe-oF targets configured:"
cat ${src_targets_yaml}

echo "inventory file:"
cat ${inventory_yaml}

ansible-playbook -i ${inventory_yaml} main.yaml -v

not_ready_nodes_summary=$(yq eval '.all.children | with_entries(select(.key | test("not_ready_.*"))) | .[].hosts | select(. != null) | keys | join(",")' ${inventory_yaml})
if [ -n "${not_ready_nodes_summary}" ]; then
  echo "WARNING: Deployment was skipped on the following NotReady node(s): ${not_ready_nodes_summary}"
  echo "Please redeploy after the node(s) back to Ready state."
fi

{% endraw %}
