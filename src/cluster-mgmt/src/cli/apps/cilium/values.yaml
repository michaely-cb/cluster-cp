ipam:
  # -- Configure IP Address Management mode.
  # ref: https://docs.cilium.io/en/stable/concepts/networking/ipam/
  mode: "cluster-pool"
  operator:
    # IPv4 CIDR range to delegate to individual nodes for IPAM.
    # it will give us 512 nodes * 64 pods/node range
    # need to revisit if nodes larger than this
    clusterPoolIPv4PodCIDRList: {}
    clusterPoolIPv4MaskSize: 0
cni:
  # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the
  # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.
  # This ensures no Pods can be scheduled using other CNI plugins during Cilium
  # agent downtime.

  # We have Multus enabled by default so disable exclusive to avoid break multus config.
  exclusive: false

extraArgs:
  - "--mtu=1500"

debug:
  # -- Enable debug logging
  enabled: true

# -- EnableHighScaleIPcache enables the special ipcache mode for high scale
# clusters. The ipcache content will be reduced to the strict minimum and
# traffic will be encapsulated to carry security identities.
# https://github.com/cilium/design-cfps/blob/main/cilium/CFP-25243-high-scale-ipcache.md
#highScaleIPcache:
  #enabled: true

# -- The agent can be put into one of the three policy enforcement modes:
# default, always and never.
# ref: https://docs.cilium.io/en/stable/security/policy/intro/#policy-enforcement-modes
policyEnforcementMode: "default"

# -- Roll out cilium agent pods automatically when configmap is updated.
# idea is update shasum as annotation which will trigger DS diff
rollOutCiliumPods: true

k8sClientRateLimit:
  # -- The sustained request rate in requests per second.
  qps: 50
  # -- The burst request rate in requests per second.
  # The rate limiter will allow short bursts with a higher rate.
  burst: 100

resources:
  # Ideally this limit would be set, but since the number of pods in the system
  # is proportional to the memory usage, we can't set a limit high enough for
  # our largest clusters.
  # limits:
  #   memory: 1Gi
  requests:
    cpu: 100m

updateStrategy:
  rollingUpdate:
    # Allow max 100 pods to be updated at the same time. The actual value is set in helm_upgrade.
    maxUnavailable: 100
  type: RollingUpdate

operator:
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 256Mi
  replicas: 1
  # -- Interval for endpoint garbage collection.
  endpointGCInterval: "5m0s"
  # -- Interval for cilium node garbage collection.
  nodeGCInterval: "5m0s"
  # -- Interval for identity garbage collection, default is 15m
  identityGCInterval: "5m0s"
  nodeSelector:
    "node-role.kubernetes.io/control-plane": ""
  # use default "tolerate everything" setting to tolerate NotReady nodes on k8 cluster init
  # tolerations: [{"Exists": true}]
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: "1m"

hubble:
  enabled: false
  # NOTE: disabled to conserve IPs on mgmt node
  # relay:
  #   enabled: false
  #   nodeSelector:
  #     "k8s.cerebras.com/node-role-management": ""
  #   tolerations:
  #     - key: node-role.kubernetes.io/master
  #       operator: Exists
  #       effect: NoSchedule
  #     - key: node-role.kubernetes.io/control-plane
  #       operator: Exists
  #       effect: NoSchedule
  #   resources:
  #     requests:
  #       cpu: 100m
  #       memory: 256Mi
  #     limits:
  #       memory: 256Mi
  # ui:
  #   enabled: true
  #   ingress:
  #     enabled: true
  #     annotations:
  #       kubernetes.io/ingress.class: nginx
  #     hosts:
  #       - hubble.{cluster}.cerebras.com
  #   nodeSelector:
  #     "k8s.cerebras.com/node-role-management": ""
  #   tolerations:
  #     - key: node-role.kubernetes.io/master
  #       operator: Exists
  #       effect: NoSchedule
  #     - key: node-role.kubernetes.io/control-plane
  #       operator: Exists
  #       effect: NoSchedule
  #   resources:
  #     requests:
  #       cpu: 100m
  #       memory: 128Mi
  #     limits:
  #       memory: 128Mi
  # metrics:
  #   enabled:
  #     - dns
  #     - drop
  #     - tcp
  #     - flow
  #     - icmp
  #     - http
  #   serviceMonitor:
  #     enabled: true

prometheus:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: "1m"

# envoy is enabled by default in 1.16
envoy:
  enabled: false