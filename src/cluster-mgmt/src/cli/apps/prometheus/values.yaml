# use a shorter name for resources
nameOverride: prometheus
fullnameOverride: prometheus

kubeEtcd:
  enabled: true
  service:
    enabled: true
    port: 2379
    targetPort: 2379
  serviceMonitor:
    scheme: https
    insecureSkipVerify: false
    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    certFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    keyFile: /etc/prometheus/secrets/etcd-client-cert/ca.key

kube-state-metrics:
  replicas: 1
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - prometheus
            topologyKey: kubernetes.io/hostname
  enabled: true
  # https://github.com/kubernetes/kube-state-metrics#resource-recommendation
  nodeSelector:
    "node-role.kubernetes.io/control-plane": ""
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  rbac:
    # If true, create & use RBAC resources
    create: true
    # Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to it, rolename set here.
    # useExistingRole: your-existing-role

    # If set to false - Run without Cluteradmin privs needed - ONLY works if namespace is also set (if useExistingRole is set this name is used as ClusterRole or Role to bind to)
    useClusterRole: true
    extraRules:
      - apiGroups: [ "apiextensions.k8s.io" ]
        resources: [ "customresourcedefinitions" ]
        verbs: [ "list", "watch" ]
      - apiGroups: [ "jobs.cerebras.com" ]
        resources: [ "wsjobs", "resourcelocks", "systems", "namespacereservations" ]
        verbs: [ "list", "watch" ]
  extraArgs:
    - --custom-resource-state-config
    # https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md
    # in YAML files, | allows a multi-line string to be passed as a flag value
    # see https://yaml-multiline.info
    - |
      spec:
        resources:
            - groupVersionKind:
                group: jobs.cerebras.com
                version: v1
                kind: WSJob
              metricNamePrefix: ""
              metrics:
                - name: wsjob_annotation
                  help: "wsjob info"
                  labelsFromPath:
                    name: [ metadata, name ]
                    namespace: [ metadata, namespace ]
                  each:
                    type: Info
                    info:
                      labelsFromPath:
                        node_groups: [ metadata, annotations, k8s.cerebras.com/wsjob-nodegroup-names ]
                        systems: [ metadata, annotations, k8s.cerebras.com/wsjob-system-names ]
                        "*": [metadata, labels]
                        # backwards compatible and can be removed later
                        job_type: [ metadata, labels, k8s.cerebras.com/wsjob-job-type ]
                        labels_k8s_cerebras_com_Model: [ metadata, labels, labels.k8s.cerebras.com/model ]
                        system_count: [ spec, numWafers ]
                        start: [ status, startTime ]
                        end: [ status, completionTime ]
                        username: [ spec, user, username ]
                        current_status: [ status, conditions, "-1", type ]
                        user_volumes: [ metadata, annotations, k8s.cerebras.com/user-volumes ]
            - groupVersionKind:
                group: jobs.cerebras.com
                version: v1
                kind: ResourceLock
              metricNamePrefix: ""
              metrics:
                - name: wsjob_lock_status
                  help: "wsjob lock status"
                  labelsFromPath:
                    name: [ metadata, name ]
                    namespace: [ metadata, namespace ]
                  each:
                    type: StateSet
                    stateSet:
                      labelName: state
                      path: [ status, state ]
                      list: [ Pending, Granted, Error ]
                - name: wsjob_system_grant
                  help: "wsjob granted systems"
                  labelsFromPath:
                    name: [ metadata, name ]
                    namespace: [ metadata, namespace ]
                  each:
                    type: Info
                    info:
                      path: [ status, systemGrants ]
                      labelsFromPath:
                        system: [ name ]
                - name: wsjob_nodegroup_grant
                  help: "wsjob granted nodegroups"
                  labelsFromPath:
                    name: [ metadata, name ]
                    namespace: [ metadata, namespace ]
                  each:
                    type: Info
                    info:
                      path: [ status, groupResourceGrants ]
                      labelsFromPath:
                        group: [ groupingValue ]
            - groupVersionKind:
                group: jobs.cerebras.com
                version: v1
                kind: System
              metricNamePrefix: ""
              metrics:
                - name: system_label
                  help: "system info"
                  labelsFromPath:
                    system: [ metadata, name ]
                  each:
                    type: Info
                    info:
                      labelsFromPath:
                        "*": [metadata, labels]
                - name: system_status_condition
                  help: "system status condition"
                  labelsFromPath:
                    system: [ metadata, name ]
                  each:
                    type: Gauge
                    gauge:
                      path: [status, conditions]
                      labelsFromPath:
                        type: ["type"]
                      valueFrom: ["status"]
            - groupVersionKind:
                group: jobs.cerebras.com
                version: v1
                kind: NamespaceReservation
              metricNamePrefix: ""
              metrics:
                - name: namespace_reservation
                  help: "namespace reservation"
                  each:
                    type: Info
                    info:
                      labelsFromPath:
                        namespace: [ metadata, name ]
                        updateMode: [ spec, requestParameters, updateMode ]
                        "*": [metadata, labels]
                - name: namespace_reserved_node_group
                  help: "namespace node group assignment"
                  labelsFromPath:
                    namespace: [ metadata, name ]
                  each:
                    type: Info
                    info:
                      path: [ status, nodegroups ]
                      labelsFromPath:
                        group: []
                - name: namespace_reserved_system
                  help: "namespace system assignment"
                  labelsFromPath:
                    namespace: [ metadata, name ]
                  each:
                    type: Info
                    info:
                      path: [ status, systems ]
                      labelsFromPath:
                        system: []
                - name: namespace_reserved_node
                  help: "namespace node assignment"
                  labelsFromPath:
                    namespace: [ metadata, name ]
                  each:
                    type: Info
                    info:
                      path: [ status, nodes ]
                      labelsFromPath:
                        node: []

prometheusOperator:
  resources:
    requests:
      cpu: 100m
    limits:
      memory: 512Mi
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  nodeSelector:
    "node-role.kubernetes.io/control-plane": ""
  admissionWebhooks:
    patch:
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      nodeSelector:
        "node-role.kubernetes.io/control-plane": ""
  prometheusConfigReloader:
    resources:
      requests:
        cpu: 200m
      limits:
        memory: 100Mi

alertmanager:
  config:
    route:
      receiver: 'default'
      group_by: [ 'alertname' ]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 24h
      routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
    receivers:
      - name: 'null'
      - name: 'default'
  alertmanagerSpec:
    replicas: 1
    podAntiAffinity: soft
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      "node-role.kubernetes.io/control-plane": ""
    resources:
      requests:
        cpu: 200m
      limits:
        memory: 200Mi
  ingress:
    enabled: true
    ingressClassName: nginx
    paths:
      - /
    tls:
      - secretName: alertmanager-general-tls

grafana:
  replicas: 1
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - prometheus
            topologyKey: kubernetes.io/hostname
  nodeSelector:
    "node-role.kubernetes.io/control-plane": ""
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 200m
    limits:
      memory: 1024Mi
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      # 2 days
      nginx.ingress.kubernetes.io/session-cookie-max-age: "172800"
      nginx.ingress.kubernetes.io/session-cookie-name: grafana
      nginx.ingress.kubernetes.io/affinity: cookie
    paths:
      - /
    tls:
      - secretName: grafana-general-tls
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: ALL
      folderAnnotation: folder
      annotations:
        folder: /tmp/dashboards/debug
      provider:
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      defaultDatasourceEnabled: false
      isDefaultDatasource: false
    alerts:
      enabled: true
      searchNamespace: ALL
      label: grafana_alert
      labelValue: "1"
  grafana.ini:
    dashboards:
      # Path to the default home dashboard. If this value is empty, then Grafana uses StaticRootPath + "dashboards/home.json"
      default_home_dashboard_path: /tmp/dashboards/cerebras-home.json
    dataproxy:
      # https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#timeout
      timeout: 120
    plugins:
      - netsage-sankey-panel
  additionalDataSources:
    - name: Prometheus
      type: prometheus
      uid: prometheus
      access: proxy
      url: http://prometheus-operated.prometheus:9090
      isDefault: true
      # https://grafana.com/docs/grafana/latest/administration/provisioning/#json-data
      # timeInterval can be override by the minStep at query level
      jsonData:
        tlsSkipVerify: true
        timeInterval: 30s
        keepCookies:
          - grafana
      # required to bypass Thanos-Query basic auth
      basicAuth: true
      secureJsonData:
        basicAuthPassword: THANOS_QUERY_PASSWORD
      basicAuthUser: THANOS_QUERY_USERNAME
    - name: Loki
      type: loki
      access: proxy
      url: http://loki.loki:3100
      jsonData:
        maxLines: 1000

prometheus:
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      # 2 days
      nginx.ingress.kubernetes.io/session-cookie-max-age: "172800"
      nginx.ingress.kubernetes.io/session-cookie-name: grafana
      nginx.ingress.kubernetes.io/affinity: cookie
    paths:
      - /
    tls:
      - secretName: prometheus-general-tls
  server:
    persistentVolume:
      enabled: true
  ## not working as expected on internal clusters, maybe related with /sys/module/xt_recent/parameters/ip_pkt_list_tot set to 0
  #service:
  #  sessionAffinity: ClientIP
  prometheusSpec:
    replicas: 1
    shards: 1
    replicaExternalLabelName: prometheus_replica
    podAntiAffinity: soft
    scrapeInterval: 30s
    evaluationInterval: 30s
    retention: 30d
    secrets: [ "etcd-client-cert" ]
    probeSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheus
    # https://kubernetes.io/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/
    # pvcs will be retained in case of helm uninstall
    # to update this to delete, feature gate needs to be turned on explicitly
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      # Don't need a toleration for `node-role-management` because (unlike `control-plane`) we don't have a taint for it
    nodeSelector:
      "k8s.cerebras.com/node-role-management": ""
    resources: { }
    volumeMounts:
      - name: snmp-target-list
        mountPath: /etc/prometheus/snmp
    volumes:
      - name: snmp-target-list
        configMap:
          name: snmp-target-list
    # Memory and CPU will be set dynamically based on the number of nodes in the cluster
    storageSpec:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 50Gi
          storageClassName: standard
    additionalScrapeConfigs:
      - job_name: 'snmp-interface-watch'
        scrape_interval: 2m
        metrics_path: /interface_watch_list
        static_configs:
          - targets:
              - 'prometheus-snmp-exporter:8080'
      - job_name: 'snmp'
        scrape_interval: 1m
        # Each switch scrape usually takes < 5 seconds. Some switches occasionally take much longer, 20s
        # or more. The snmp_exporter process stops a scrape attempt and returns an HTTP error after 30s.
        scrape_timeout: 50s
        file_sd_configs:
          - files:
              - "/etc/prometheus/snmp/snmp-sd.json"
        metrics_path: /snmp
        params:
          module: [ if_mib ]
        relabel_configs:
          - source_labels: [ __address__ ]
            target_label: __param_target
          - source_labels: [ __param_target ]
            target_label: instance
          - source_labels: [ __param_target ]
            target_label: switch_id
          - source_labels: [ snmp_exporter_host ]
            target_label: __address__

nodeExporter:
  enabled: true

prometheus-node-exporter:
  resources:
    requests:
      cpu: 500m
      memory: 256Mi
    limits:
      memory: 256Mi
  updateStrategy:
    rollingUpdate:
      maxUnavailable: "34%"
    type: RollingUpdate
  prometheus:
    monitor:
      metricRelabelings:
        # Drop metrics from interfaces we don't care about.
        # See net_json_to_configmap.sh for the corresponding interface dropping for user nodes.
        - sourceLabels: [device]
          regex: (lo|lxc|mac|nerdctl|docker|cilium|idrac).*
          action: drop
  extraArgs:
    - --collector.cpu.info
    - --collector.netdev.address-info


thanosRuler:
  enabled: false
  ingress:
    enabled: true
    ingressClassName: nginx
    paths:
      - /
    tls:
      - secretName: ruler-general-tls
  thanosRulerSpec:
    retention: 30d
    ruleSelectorNilUsesHelmValues: false
    evaluationInterval: 30s
    nodeSelector:
      "node-role.kubernetes.io/control-plane": ""
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    resources:
      requests:
        cpu: 200m
        memory: 1Gi
    ruleSelector:
      matchExpressions:
        - key: localEval
          operator: DoesNotExist
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-filesystem
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 100Gi
    # required to bypass Thanos-Query basic auth
    queryConfig:
      secret:
        - http_config:
            basic_auth:
              username: THANOS_QUERY_USERNAME
              password: THANOS_QUERY_PASSWORD
          static_configs:
            - "thanos-query.prometheus:9090"
    alertmanagersConfig:
      secret:
       alertmanagers:
       - api_version: v2
         static_configs:
           - "dnssrv+_http-web._tcp.alertmanager-operated.prometheus"
