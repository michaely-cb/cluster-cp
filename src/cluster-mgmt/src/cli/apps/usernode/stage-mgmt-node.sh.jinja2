#!/usr/bin/env bash

# This script is run as part of the package installer on the mgmt node. It places
# artifacts in the correct locations so that during the usernode install, the
# usernode.sh script can fetch from the mgmt node and install the user package.

cd $(dirname "$0")
source "../pkg-common.sh"

set -e

if [ -z "$namespace" ]; then
  namespace="{{ namespace }}"
fi
system_namespace="{{ system_namespace }}"
overwrite_usernode_configs="{{ overwrite_usernode_configs }}"
reset_usernode_configs="{{ reset_usernode_configs }}"
update_config_only="{{ update_config_only }}"
skip_install_default_csctl="{{ skip_install_default_csctl }}"
enable_usernode_monitoring="{{ enable_usernode_monitoring }}"
BIN_DIR="/usr/local/bin"
KUBECTL_SYSTEM="$KUBECTL -n ${system_namespace}"
KUBECTL_PROM="$KUBECTL -n prometheus"

function prepare_package() {
    # Create configuration file which will be used by the usernode which loads this package.
    # Ensure to use system NS authority to keep user node setup consistent
    local base_authorities=($($KUBECTL_SYSTEM get ingress cluster-server -ojson | jq -r '.spec.rules[] | .host // empty'))
    if [[ ! "$base_authorities" ]]; then
        echo "error: cluster-server ingress base authorities not found, was cluster-server deployed in the system namespace?" >&2
        return 1
    fi

    # load balancer may not be ready if this is run immediately after cluster-server is deployed
    if ! timeout 120 bash -c -- "
    while true ; do
        if $KUBECTL_SYSTEM get ingress cluster-server --ignore-not-found=true -ojsonpath='{.status.loadBalancer.ingress}' | grep -w $MGMT_NODE_DATA_IP; then
            break
        fi
        sleep 5
    done
    "; then
        echo "error: cluster-server ip not found, was cluster-server deployed in the system namespace?" >&2
        return 1
    fi

    local has_vip=$({ { has_multiple_mgmt_nodes && has_data_network ; } || [ -d /kind ] ; } && echo "true" || echo "false")
    local is_kind=$([ -d /kind ] && echo "true" || echo "false")
    local mgmt_ip=$(get_mgmt_data_ip)

    local cwd=$(pwd -P)
    local config_v2="$cwd/config_v2"
    local install_json_path="$1"
    local install_json
    if [ "$reset_usernode_configs" == "true" ]; then
        generate_csctl_cfg "$config_v2" ""
    else
        local ingress=$($KUBECTL -n "${namespace}" get ingress cluster-server --ignore-not-found)
        if [ -z "$ingress" ]; then
            echo "error: cluster-server ingress in namespace '${namespace}' not found, was cluster-server deployed?" >&2
            exit 1
        fi

        generate_csctl_cfg "$config_v2" "$namespace"
    fi

    # cluster specific config for the usernode installer to use
    install_json=$(cat << EOF
{
    "kind": "UsernodeInstallConfiguration",
    "ip": "$mgmt_ip",
    "domains": [],
    "certs": {},
    "managementDataIPs": {},
    "overwriteUsernodeConfigs": "$overwrite_usernode_configs",
    "updateConfigOnly": "$update_config_only",
    "skipInstallDefaultCsctl": "$skip_install_default_csctl",
    "enableUsernodeMonitoring": "$enable_usernode_monitoring",
    "isKind": "$is_kind"
}
EOF
)

    if $has_vip ; then
        # Ignore any management nodes that are in error state. Previous checking on NotReady state is not enough.
        # The node can be in Ready state, but its 100G link can be flapping.
        ignore_nodes=$(csctl get cluster -ojson | \
            jq -r '.items[] | select(.state == "error" and (.role == "management" or .role == "coordinator" or .role == "any")) | .meta.name')
        while read -r line ; do
            node="$(cut -d, -f1 <<< $line)"
            addr="$(cut -d, -f2 <<< $line)"
            if grep -qw "$node" <<< "$ignore_nodes"; then
                continue
            fi
            install_json=$(
                jq --arg name "$node" \
                   --arg addr "$addr" \
               '.managementDataIPs[$addr] = $name' <<< "$install_json")
        done <<< $(yq -r '.nodes[] | select(.role == "management" or .role == "coordinator") | [.name,.networkInterfaces[0].address // ""] | @csv' ${CLUSTER_CONFIG})
    fi

    # keep for backwards compatible reasons
    # Domains are namespace agnostic
    local cluster_server_domains=($($KUBECTL_SYSTEM get ingress cluster-server -ojson | jq -r '.spec.tls[0].hosts[]'))
    for url in "${cluster_server_domains[@]}" ; do
        install_json=$(jq -r --arg url "$url" '.domains += [$url]' <<< "$install_json")
    done

    # Add grafana_tls.crt to certs in install.json and add grafana_host to domains.
    # The installer will not break if this is not present.
    local grafana_cfg=$($KUBECTL_PROM get ingress prometheus-grafana -ojsonpath='{.spec.tls[0]}')
    local grafana_host=$(echo "$grafana_cfg" | jq -r '.hosts[0]')
    local grafana_crt=$($KUBECTL_PROM get secret $(echo "$grafana_cfg" | jq -r '.secretName') -ojsonpath='{.data.tls\.crt}')
    if [ "$?" == "0" ] && [ -n "$grafana_crt" ] ; then
        install_json=$(
            jq -r --arg grafana_host "$grafana_host" \
                  --arg grafana_crt "$grafana_crt" \
              '.domains += [$grafana_host] | .certs += {"grafana_tls.crt": $grafana_crt}' <<< "$install_json")
    else
        echo "warning: grafana cert not found, was prometheus deployed?" >&2
    fi

    # Add multus + cilium virtual ranges to usernode for usernode-side validation
    virtual_ranges=$(yq -ojson ${CLUSTER_CONFIG} |
        jq -r '[
                (.groups[] | [(.switchConfig.virtualStart // "xxx"), (.switchConfig.virtualEnd // "xxx")]) ,
                ((.v2Groups // [])[] | .vlans[] | [(.virtualStart // "xxx"), (.virtualEnd // "xxx")])
        ] | .[] | @csv' |
        grep -v 'xxx' |
        tr -d '"' |
        sort |
        uniq)
    for range in $virtual_ranges ; do
        if [ -z "$range" ] ; then continue ; fi
        install_json=$(
            jq -r --arg vstart "$(cut -d, -f1 <<<"$range")" \
                  --arg vend "$(cut -d, -f2 <<<"$range")" \
              '.reservedIpRanges += [ [ $vstart, $vend ] ]' <<< "$install_json")
    done
    local cilium_range=$($KUBECTL get cm -n kube-system cilium-config -ojsonpath='{.data.cluster-pool-ipv4-cidr}' --ignore-not-found)
    if [ -n "$cilium_range" ] && which python3 &>/dev/null ; then
        local cilium_start="${cilium_range%/*}"
        local cilium_end=$(python3 -c "from ipaddress import ip_network ; print(ip_network(\"$cilium_range\").broadcast_address)")
        install_json=$(
            jq -r --arg vstart "$cilium_start" \
                  --arg vend "$cilium_end" \
              '.reservedIpRanges += [ [ $vstart, $vend ] ]' <<< "$install_json")
    fi
    echo "$install_json" > "$install_json_path"

    # Add the config and config_v2 file to the package base-user-pkg-<version>.tar.gz
    # This customizes the user package specially for this cluster.
    local basePkgFileTarGz=$(ls -1 base-user-pkg-*.tar.gz)
    if [ -z "$basePkgFileTarGz" ] ; then
        echo "error: user package not found" >&2
        return 1
    fi
    local version=$(basename "$basePkgFileTarGz" | sed -e 's/^base-user-pkg-//' -e 's/\.tar\.gz$//')

    local basePkgFileTar="${basePkgFileTarGz%.gz}"
    basePkgFile="${basePkgFileTarGz%.tar.gz}"
    pkgFile="${basePkgFile#base-}"
    pkgFileTarGz="$pkgFile.tar.gz"

    echo "adding config to ${pkgFileTarGz} (version=$version)"

    pushd $(mktemp -d) >/dev/null
    local authSecret=$($KUBECTL_SYSTEM get secret kube-user-auth -ojsonpath='{.data.secret}' | base64 --decode -w 0)
    user_auth_secret_tmp=$(mktemp)
    echo -n "$authSecret" > "$user_auth_secret_tmp"
    cp "$user_auth_secret_tmp" "$cwd/user-auth-secret"
    $MKDIR -p "$USER_AUTH_SECRET_DIR"
    $CP "$user_auth_secret_tmp" "$USER_AUTH_SECRET_DIR/user-auth-secret"

    tar xfz "${cwd}/${basePkgFileTarGz}" -C "${cwd}"
    mv "${cwd}/${basePkgFile}" "${cwd}/${pkgFile}"

    # "csctl version 2.0.2-202310292240-20-0d7a2e0e+0d7a2e0eec" -> "2.0"
    # "csctl version 0.0.0-johndoe+c481ba92cd" -> "0.0"
    chmod 755 "$cwd/${pkgFile}/csctl"
    csctl_version=$($cwd/${pkgFile}/csctl --version | awk '{ print $3 }' | cut -d. -f1-2)

    # Renaming csctl so that the usernode deploy script can find it more easily
    cp $cwd/${pkgFile}/csctl $cwd/${pkgFile}/csctl_executable

    # csctl is also running on mgmt node so it needs to be copied. We also need to copy user-auth
    # related files on all mgmt nodes. We only do this when the user node package is running with
    # system_namespace. This is to prevent the overwrite of csctl with an older version which is
    # not compatible with the one running on the system namespace. We assume that system namespace
    # has the later version of the cluster-mgmt pieces.
    if [ "$namespace" == "$system_namespace" ]; then
        PSSH_MGMT "$MKDIR -p ${BIN_DIR}"

        # On compilepod, management node is also the user node. Don't update get-cerebras-token if
        # the version is the same. This is to avoid the `text file busy` error when get-cerebras-token
        # is still being used by running jobs.
        local token_bin_exists=false
        if [ -f "$BIN_DIR/get-cerebras-token" ]; then
            chmod u+x "$cwd/${pkgFile}/get-cerebras-token"
            local old_version=$($BIN_DIR/get-cerebras-token version)
            local new_version=$($cwd/${pkgFile}/get-cerebras-token version)
            # Before 2.0, there is no version support in get-cerebras-token. Assuming it is v1.
            if [[ $old_version == Token* ]]; then
                old_version="version v1"
            fi

            if [ "$old_version" = "$new_version" ]; then
              echo "get-cerebras-token already exists in $BIN_DIR"
              token_bin_exists=true
            fi
        fi
        if ! $token_bin_exists; then
            token_tmp_file=$(mktemp)
            PSCP_MGMT "$cwd/${pkgFile}/get-cerebras-token" "$token_tmp_file"
            PSSH_MGMT "$MV $token_tmp_file ${BIN_DIR}/get-cerebras-token"
            PSSH_MGMT "$CHMOD 4751 ${BIN_DIR}/get-cerebras-token"
        fi

        # For non-root deployment, PSCP_MGMT won't work when copying to a remote directory that is not
        # writable by the user. We write to a tmp file first and move it with PSSH_MGMT.
        csctl_tmp_file=$(mktemp)
        PSCP_MGMT "$cwd/${pkgFile}/csctl_executable" "$csctl_tmp_file"
        PSSH_MGMT "$CHMOD a+x $csctl_tmp_file && $MV $csctl_tmp_file ${BIN_DIR}/csctl"
        PSSH_MGMT "$CP ${BIN_DIR}/csctl ${BIN_DIR}/csctl${csctl_version}"
        config_tmp_file=$(mktemp)
        PSCP_MGMT "$config_v2" "$config_tmp_file"
        PSSH_MGMT "$CHMOD a+r $config_tmp_file && $MV $config_tmp_file $CFG_DIR/config_v2"
    elif [ "${csctl_version}" == "0.0" ] && [ "$overwrite_usernode_configs" != "true" ]; then
        # Remove csctl in the user package to avoid overwritting the csctl installed by the one
        # with the system namespace.
        rm -f "$cwd/${pkgFile}/csctl_executable"
    else
        csctl_tmp_file=$(mktemp)
        PSCP_MGMT "$cwd/${pkgFile}/csctl_executable" "$csctl_tmp_file"
        PSSH_MGMT "$CHMOD a+x $csctl_tmp_file && $MV $csctl_tmp_file ${BIN_DIR}/csctl${csctl_version}"
        # We should not copy the configs to other mgmt nodes if the deployment wasn't targeting
        # the system namespace.
    fi

    cp "$config_v2" "${cwd}/${pkgFile}/config_v2" && \
        cp "$install_json_path" "${cwd}/${pkgFile}/install.json" && \
        cp "$cwd/user-auth-secret" "${cwd}/${pkgFile}/user-auth-secret" && \
        tar -czh -C "${cwd}" "${pkgFile}" > "${cwd}/${pkgFileTarGz}"  && \
        chmod 0640 "$cwd/${pkgFileTarGz}" && \
        rm -rf "${cwd}/${pkgFile}"
    popd >/dev/null
}


function copy_package() {
    local install_json_path=$1
    local pkgFileTarGz=$(ls -1 user-pkg-*.tar.gz)
    if [ -z "$pkgFileTarGz" ] ; then
        echo "error: user package not found" >&2
        return 1
    fi

    # For non-root deployment, make $pkgFileTarGz readable
    chmod a+r "$pkgFileTarGz"
    $MKDIR -p "$PKG_DIR"
    $CP "$pkgFileTarGz" "$PKG_DIR"
    $CHMOD 0640 "$PKG_DIR/$pkgFileTarGz"
    $SUDO_BASH "cat <<EOF > \"$PKG_DIR/user-pkg-readme\"
User node appliance package installation

$PKG_DIR/$pkgFileTarGz

The above file contains installation packages configured for this cluster.
Do not use user-pkg-<version>.tar.gz files from elsewhere in the filesystem as
they will be missing csadm-generated configuration files necessary for
installation.

If you are configuring the usernode's IPs, note that the following
IP ranges are reserved by the cluster. Do not assign IPs from within these
ranges to your usernode:
$(jq -r '.reservedIpRanges[] | @csv' $install_json_path | sed 's/,/ to /' | tr -d '"' | sed 's/^/  /')

The recommended way to install the user node package is as follows:

1. Copy $pkgFileTarGz to the user node using scp or similar

   scp $PKG_DIR/$pkgFileTarGz <user>@<user-node-ip>:$PKG_DIR/

2. On the user node, run the following commands as root

   cd $PKG_DIR
   tar xzf $pkgFileTarGz
   cd ${pkgFileTarGz%.tar.gz}/ && ./install.sh

On the usernode, you can run install.sh --help for more details.
EOF
    "
}

install_json_path=$(mktemp)
if ! prepare_package "$install_json_path" ; then
    echo "error: failed to create package" >&2
    exit 1
fi

if [[ "$1" != "--skip-copy" ]] ; then
    copy_package "$install_json_path"
fi
