#!/usr/bin/env bash

SKIP_IF_INCREMENTAL=1

SCRIPT_PATH=$(dirname "$0")
cd "$SCRIPT_PATH"
source "../pkg-common.sh"

if ! should_install_multus ; then
  echo "Cluster is not configured for multiple networks, not installing multus and whereabouts"
  exit 0
fi

set -e

get_unreachable_nodes

read max_pods_unavailable num_batches <<< $(get_ds_max_unavailable)
total_seconds=$(echo "$num_batches * 60" | bc)

# TODO: remove lock creation + locks in 2.6 as they are no longer used with 2.5 whereabouts
# create a lease for each subnet in the cluster-config, e.g. groups[].switchConfig.subnet = 10.0.10.0/24 ->
# whereabouts-10.0.10.0-24 . This allows our custom whereabouts fork to lock IPs by range rather than using
# a global lock. This feature can be turned off by deleting the custom leases.

subnets=$(yq '.v2Groups[] | .vlans[] | .subnet' ${CLUSTER_CONFIG} | sort | uniq)
if [ -z "${subnets}" ]; then
  subnets=$(yq '.groups[] | .switchConfig.subnet' ${CLUSTER_CONFIG} | sort | uniq)
fi
for subnet in ${subnets} ; do
  if [ "$subnet" == "null" ] ; then continue ; fi

  safe_subnet=$(tr '/' '-' <<< "$subnet")
  lease_name="whereabouts-${safe_subnet}"
  if kubectl get lease -n kube-system $lease_name &>/dev/null ; then
    continue
  fi
  cat <<EOF | kubectl apply -f-
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  labels:
    app: whereabouts
    nodegroup-lock: "true"
  annotations:
    subnet: $subnet
  name: $lease_name
  namespace: kube-system
spec: {}
EOF
  echo "created whereabouts subnet lease $lease_name"
done

if [ "{{ force }}" == "true" ] && helm -n kube-system list | grep -wq whereabouts; then
  helm -n kube-system uninstall whereabouts --wait
fi

# In 2.4, whereabouts cronjob is not needed any more. Also, k8s upgrade updates the cronjob version.
# whereabouts helm upgrade below will fail due to that. We will remove whereabouts helm secret if
# whereabouts cronjob exists.
if kubectl -n kube-system get cronjob whereabouts 2>/dev/null; then
  kubectl -n kube-system delete cronjob whereabouts
  kubectl -n kube-system delete secret -lname=whereabouts,owner=helm
fi

operator_replicas=1
if has_multiple_mgmt_nodes; then
  operator_replicas=2
fi

additional="--set operator.replicas=${operator_replicas} "
additional+="--set rollingUpdate.maxUnavailable=${max_pods_unavailable} "
helm upgrade --install whereabouts charts/whereabouts --debug --namespace kube-system \
  -f charts/whereabouts/values.yaml ${additional}
await_ds_ready kube-system whereabouts ${total_seconds} unreachable_ok

if [ "{{ force }}" == "true" ] && helm -n kube-system list | grep -wq multus; then
  helm -n kube-system uninstall multus --wait
fi

primary_cni="cilium"
readiness_indicator=$(find /etc/cni/net.d -mindepth 1 -maxdepth 1 -name '*-cilium.conf*' | head -1)
if [ -d /kind ]; then
  primary_cni="kindnet"
  readiness_indicator=$(find /etc/cni/net.d -mindepth 1 -maxdepth 1 -name '*-kindnet.conf*' | head -1)
fi

if [ -z "$readiness_indicator" ]; then
  echo "ERROR: Readiness indicator is not available for primary CNI $primary_cni. Please deploy the primary CNI first."
  exit 1
fi

master_config_filename=$(basename $readiness_indicator)

additional="--set rollingUpdate.maxUnavailable=${max_pods_unavailable} "
additional+="--set multusConfig.readinessIndicator=${readiness_indicator} "
additional+="--set multusConfig.masterConfigFilename=${master_config_filename} "
helm upgrade --install multus charts/multus --debug --namespace kube-system \
  -f charts/multus/values.yaml ${additional}
await_ds_ready kube-system multus-multus-ds ${total_seconds} unreachable_ok

cat <<EOF | kubectl apply -f-
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: ${primary_cni}
  namespace: kube-system
spec:
  config: ""
EOF
